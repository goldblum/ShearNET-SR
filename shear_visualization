import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F
import math
import numpy as np

def calculate_padding(input_size, output_size, stride, kernel_size, dilation):
	return int(((output_size - 1)*stride - input_size + kernel_size + (kernel_size - 1)*(dilation - 1))/2)

def dilate_weights(weights, dilation_factor, kernel_size, device):
	if dilation_factor == 1:
		return weights
	else: 
		A = np.identity(kernel_size)
		for i in range(kernel_size - 1):
			for j in range(dilation_factor - 1):
				A = np.insert(A, i + 1 + i*(dilation_factor - 1) + j, np.zeros(kernel_size), axis = 1)
		A = torch.from_numpy(A).float().to(device)
		#print("dilated weights")
		#print(torch.matmul(torch.t(A), torch.matmul(weights, A)))
		return torch.matmul(torch.t(A), torch.matmul(weights, A))

def shear_helper(kernel_size, direction, shift):
	sub_shear = np.zeros((kernel_size,kernel_size))
	if direction == "none" or shift == 0:
		return np.identity(kernel_size)
	if direction == "up":
		for i in range(kernel_size - shift):
			sub_shear[i, i + shift] = 1
	else:
		for i in range(kernel_size - shift):
			sub_shear[i + shift, i] = 1
	return sub_shear


def shear_weights(weights, kernel_size, direction, shift, transpose, device, pad = True):
	if shift == 0:
		return weights
	new_weights = weights.clone()
	
	
	if transpose:	
		new_weights = torch.transpose(new_weights, 2,3).contiguous()
	
	if pad:	
		new_weights = F.pad(new_weights, (shift,shift, shift, shift), "constant", 0)
		kernel_size = kernel_size + 2*shift
	while len(direction) < kernel_size: 
		direction.append("none")
		direction.insert(0, "none")
	Shear = np.zeros((kernel_size*kernel_size,kernel_size*kernel_size))
	for i in range(kernel_size):
		Shear[i*kernel_size:(i+1)*kernel_size, i*kernel_size:(i+1)*kernel_size] = shear_helper(kernel_size, direction[i], shift)
	Shear = torch.from_numpy(Shear).float().to(device)
	new_weights = new_weights.view(new_weights.shape[0], new_weights.shape[1], new_weights.shape[2] ** 2)
	if transpose:
		return torch.transpose(torch.matmul(new_weights, Shear).view(new_weights.shape[0], new_weights.shape[1], kernel_size, kernel_size), 2,3)
	else:
		return torch.matmul(new_weights, Shear).view(new_weights.shape[0], new_weights.shape[1], kernel_size, kernel_size)

def shear_manager(shear_num, dilation):
	direction = [["none"], ["up", "none", "down"],["down", "none", "up"],["up", "none", "down"],["down", "none", "up"]]
	dilated_direction = [["none"], ["up","none", "none","none", "down"],["down" , "none", "none", "none", "up"],["up","none", "none","none", "down"],["down" , "none", "none", "none", "up"]]
	shift = [0,1,1,1,1]
	transpose = [False, False, False, True, True]
	if dilation:
		return dilated_direction[shear_num], shift[shear_num], transpose[shear_num]
	return direction[shear_num], shift[shear_num], transpose[shear_num]
	

